{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_dir = \"../all/\"  # point this to the directory with all the CSV files for each station, both model and observations\n",
    "os.makedirs(\"data\", exist_ok=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather metadata about each station necessary to perform the analysis\n",
    "If it hasn't been stored yet, calculate it and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/metadata.json\"):\n",
    "    with open(\"data/metadata.json\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "else:\n",
    "    metadata = dict()\n",
    "    start_date = np.datetime64(\"2018-01-01\")  # sometimes there's data from just before jan 1 2018, which we want to exclude\n",
    "    fs = [s for s in os.listdir(station_dir) if s.endswith(\".csv\")]\n",
    "    for i, station_fname in tqdm(enumerate(fs), total=len(fs)):\n",
    "        if station_fname not in metadata:\n",
    "            metadata[station_fname] = dict()\n",
    "        if station_fname.startswith(\"obs\"):\n",
    "            pathname = os.path.join(station_dir, station_fname)\n",
    "            df = pd.read_csv(pathname, parse_dates=['ISO8601',], date_parser=lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ')).sort_values(by=['ISO8601'])\n",
    "            df = df[df.ISO8601 >= start_date]\n",
    "            df = df[df.obstype=='no2']\n",
    "            if len(df) == 0:\n",
    "                metadata[station_fname] = dict(obs=station_fname.startswith(\"obs\"), n_data=0, quality=np.nan, lat=np.nan, lon=np.nan, mean=np.nan, std=np.nan, max=np.nan, min=np.nan)\n",
    "                continue\n",
    "            metadata[station_fname][\"obs\"] = True\n",
    "            metadata[station_fname][\"mean\"] = df.conc_obs.mean()\n",
    "            metadata[station_fname][\"std\"] = df.conc_obs.std()\n",
    "            metadata[station_fname][\"max\"] = df.conc_obs.max()\n",
    "            metadata[station_fname][\"min\"] = df.conc_obs.min()\n",
    "            station_name = station_fname[4:-4]\n",
    "\n",
    "        if station_fname.startswith(\"model_forecast\"):\n",
    "            pathname = os.path.join(station_dir, station_fname)\n",
    "            df = pd.read_csv(pathname, parse_dates=['ISO8601',], date_parser=lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ')).sort_values(by=['ISO8601'])\n",
    "            df.ISO8601 -= dt.timedelta(minutes=30)  # the timestamps in mod were misaligned\n",
    "            df = df[df.ISO8601 >= start_date]\n",
    "            if len(df) == 0:\n",
    "                metadata[station_fname] = dict(obs=station_fname.startswith(\"obs\"), n_data=0, quality=np.nan, lat=np.nan, lon=np.nan, mean=np.nan, std=np.nan, max=np.nan, min=np.nan)\n",
    "                continue\n",
    "            metadata[station_fname][\"obs\"] = False\n",
    "            metadata[station_fname][\"mean\"] = df.NO2.mean()\n",
    "            metadata[station_fname][\"std\"] = df.NO2.std()\n",
    "            metadata[station_fname][\"max\"] = df.NO2.max()\n",
    "            metadata[station_fname][\"min\"] = df.NO2.min()\n",
    "            station_name = station_fname[len(\"model_forecast_\"):-4]\n",
    "\n",
    "        metadata[station_fname][\"station\"] = station_name\n",
    "        metadata[station_fname][\"n_data\"] = len(df)\n",
    "        metadata[station_fname][\"quality\"] = len(df) / ((df.ISO8601.iloc[-1] - df.ISO8601.iloc[0]).days * 24 + 23)\n",
    "        metadata[station_fname][\"lat\"] = df.iloc[0].lat\n",
    "        metadata[station_fname][\"lon\"] = df.iloc[0].lon\n",
    "        if i % 100 == 0:\n",
    "            with open(\"data/metadata.json\", \"w\") as f:\n",
    "                f.write(json.dumps(metadata))\n",
    "\n",
    "    with open(\"data/metadata.json\", \"w\") as f:\n",
    "        f.write(json.dumps(metadata))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign locations to regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_thresh = 0.5\n",
    "obs_thresh = 10\n",
    "region_centers =  {\"birmingham\": [[52.472927], [-1.902408]], \"manchester\": [[53.490899], [-2.234560]], \"london\": [[51.506722], [-0.178576]],\n",
    "                    \"wuhan\": [[30.59], [114.31]], \"milan\": [[45.46], [9.19]], \"losangeles\": [[34.05], [-118.24]],\n",
    "                    \"paris\": [[48.867190], [2.349779]], \"seattle\": [[47.578678], [-122.301561]],\n",
    "                    \"madrid\": [[40.408597], [-3.698773]], \n",
    "                    \"berlin\": [[52.488120], [13.410967]], \"barcelona\": [[41.396556], [2.164114]], \"santiago\": [[-33.463444], [-70.638429]],\n",
    "                    \"beijing\": [[39.929069], [116.363086]], \"shanghai\": [[31.185547], [121.526011]], \"tokyo\": [[35.676772], [139.827864]],\n",
    "                    \"hongkong\": [[22.336816], [114.158563]], \"teipei\": [[25.017115], [121.538235]], \"bangkok\": [[13.773331], [100.581897]],\n",
    "                    \"jerusalem\": [[31.760589], [35.213318]], \"newyork\": [[40.683845], [-73.988544]]}\n",
    "\n",
    "region_names = set(region_centers.keys())\n",
    "regions = dict()\n",
    "\n",
    "# assign each station to a region based on euclidean distance from a city center\n",
    "for region in region_names:\n",
    "    regions[region] = dict()\n",
    "    for station_fname in metadata.keys():\n",
    "        lat = metadata[station_fname][\"lat\"]\n",
    "        lon = metadata[station_fname][\"lon\"]\n",
    "        if (lat - region_centers[region][0][0])**2 + (lon - region_centers[region][1][0])**2 < 0.5**2:\n",
    "            regions[region][station_fname] = metadata[station_fname]\n",
    "\n",
    "[(r, len(regions[r])) for r in regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    for station_fname in regions[region].keys():\n",
    "        s = regions[region][station_fname]\n",
    "        if s[\"obs\"] and s[\"quality\"] > quality_thresh and s[\"mean\"] > obs_thresh:\n",
    "            station_name = station_fname[4:-4]\n",
    "            mod_fname = f\"model_forecast_{station_name}.csv\"\n",
    "\n",
    "            metadata[mod_fname][\"station\"] = station_name\n",
    "            metadata[station_fname][\"station\"] = station_name\n",
    "            metadata[mod_fname][\"clusters\"] = metadata[mod_fname].get(\"clusters\", []) + [region]\n",
    "            metadata[station_fname][\"clusters\"] = metadata[station_fname].get(\"clusters\", []) + [region]\n",
    "\n",
    "for station_fname in metadata:\n",
    "    if \"clusters\" not in metadata[station_fname].keys():\n",
    "        metadata[station_fname][\"clusters\"] = []\n",
    "        \n",
    "for row in metadata.values():\n",
    "    if any([i in {\"madrid\", \"barcelona\"} for i in row[\"clusters\"]]):\n",
    "        row[\"clusters\"].append(\"madrid-barcelona\")\n",
    "for row in metadata.values():\n",
    "    if any([i in {\"beijing\", \"shanghai\", \"wuhan\"} for i in row[\"clusters\"]]):\n",
    "        row[\"clusters\"].append(\"beijing-shanghai-wuhan\")\n",
    "\n",
    "\n",
    "with open(\"data/metadata.json\", \"w\") as f:\n",
    "    f.write(json.dumps(metadata))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DPK to get forecasts for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import station_forecast\n",
    "\n",
    "# this will take a while\n",
    "station_forecast.run(station_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection for a particular region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 633\n",
    "\n",
    "print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from dpk.koopman_probabilistic import KoopmanProb\n",
    "from dpk.model_objs import NormalNLL\n",
    "\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "from scipy import interpolate\n",
    "import time\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the data for the stations in this cluster, preprocess/take log, and put into multidim array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(cluster_id, quality_thresh):\n",
    "    with open(\"data/metadata.json\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "\n",
    "    temp = []\n",
    "    for fname in metadata:\n",
    "        temp.append(metadata[fname])\n",
    "        temp[-1][\"fname\"] = fname\n",
    "    df_meta = pd.DataFrame(temp)\n",
    "    stations = df_meta.loc[[(cluster_id in r.clusters and r.quality > quality_thresh) for r in df_meta.iloc]].station.unique()\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_scores(x, t, koop):\n",
    "    params = koop.predict(t, covariates=t.reshape(len(t), 1))\n",
    "    mean_hat = koop.model_obj.mean(params)\n",
    "    std_hat = koop.model_obj.std(params)\n",
    "    z_scores = (x - mean_hat) / std_hat\n",
    "    return z_scores\n",
    "\n",
    "def get_contiguous_k_samples(k, series):\n",
    "    if len(series) < k:\n",
    "        return np.zeros((0, k))\n",
    "    k_samples = np.zeros((len(series) - k + 1, k))\n",
    "    for i in range(0, len(series) - k + 1):\n",
    "        sample = series[i:i+k]\n",
    "        k_samples[i, :] = sample.flatten()\n",
    "\n",
    "    return k_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(train_ts, train_k_averages, lamb):\n",
    "    sorted_train_k_averages = sorted(train_k_averages)\n",
    "    len_tr = len(sorted_train_k_averages)\n",
    "    IQR = sorted_train_k_averages[3 * len_tr // 4] - sorted_train_k_averages[len_tr // 4]\n",
    "    lower_thresh, upper_thresh = sorted_train_k_averages[len_tr // 4] - lamb * IQR, sorted_train_k_averages[3 * len_tr // 4] + lamb * IQR\n",
    "\n",
    "    mask = (lower_thresh < train_k_averages) & (train_k_averages < upper_thresh)\n",
    "    return train_ts[mask], train_k_averages[mask]\n",
    "\n",
    "def load_z_scores(data_name):\n",
    "    param_str = None\n",
    "    for fname in os.listdir(\"forecasts\"):\n",
    "        if data_name in fname and fname.startswith(\"koop\"):\n",
    "            param_str = fname[5:-4]\n",
    "            print(fname)\n",
    "    if param_str is None:\n",
    "        raise FileNotFoundError(data_name)\n",
    "\n",
    "    koop = pickle.load(open(f\"forecasts/koop_{param_str}.pkl\", 'rb'))\n",
    "    x = np.load(f\"forecasts/x_{param_str}.npy\")\n",
    "    t = np.load(f\"forecasts/t_{param_str}.npy\")\n",
    "    return t, get_z_scores(x, t, koop).flatten()\n",
    "\n",
    "def get_zeta_series(stations, k=168, lamb=2, show=False):\n",
    "    # !!! make sure these match the data\n",
    "    train_start_date = dt.datetime(2018, 1, 1) # dt.datetime(2018, 3, 16)\n",
    "    train_end_date = dt.datetime(2020, 1, 1)  # covid start date dt.datetime(2020, 3, 16) for Seattle and LA, 1/23 for Wuhan, and 3/9 in Italy\n",
    "    test_end_date = dt.datetime(2021, 1, 1) # dt.datetime(2020, 5, 16)\n",
    "    t_min = time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "    train_start_t = time.mktime(train_start_date.timetuple()) - t_min\n",
    "    train_end_t = time.mktime(train_end_date.timetuple()) - t_min\n",
    "    test_end_t = time.mktime(test_end_date.timetuple()) - t_min\n",
    "\n",
    "    train_k_averages = []\n",
    "    train_ts = []\n",
    "    test_k_averages = []\n",
    "    test_ts = []\n",
    "    for i, station in enumerate(stations):        \n",
    "        # load the koopman model and relevant data for station\n",
    "        obs_t, obs_z_scores = load_z_scores(f\"obs_{station}\")\n",
    "\n",
    "        # model_analys\n",
    "        mod_t, mod_z_scores = load_z_scores(f\"model_forecast_{station}\")\n",
    "\n",
    "        train_start = np.nonzero(obs_t >= train_start_t)[0].min()\n",
    "        train_end = np.nonzero(obs_t >= train_end_t)[0].min()\n",
    "        test_end = np.nonzero(obs_t <= test_end_t)[0].max()\n",
    "\n",
    "        f = interpolate.interp1d(mod_t, mod_z_scores)\n",
    "        aligned_mod_z_scores = f(obs_t)\n",
    "        zeta_scores = obs_z_scores - aligned_mod_z_scores\n",
    "        train_k_averages.append(get_contiguous_k_samples(k, zeta_scores[train_start:train_end]).mean(axis=1))\n",
    "        train_ts.append(get_contiguous_k_samples(k, obs_t[train_start:train_end]).max(axis=1))  # represents the \"real\" time\n",
    "        test_k_averages.append(get_contiguous_k_samples(k, zeta_scores[train_end:test_end]).mean(axis=1))\n",
    "        test_ts.append(get_contiguous_k_samples(k, obs_t[train_end:test_end]).max(axis=1))\n",
    "\n",
    "        # remove outliers from training data\n",
    "        train_ts[-1], train_k_averages[-1] = remove_outliers(train_ts[-1], train_k_averages[-1], lamb=lamb)\n",
    "        \n",
    "    if show:\n",
    "        plt.figure(dpi=500, figsize=(15, 15))\n",
    "        for i in range(min(len(stations), 9)):\n",
    "            plt.subplot(3, 3, i+1)\n",
    "            plt.hist(train_k_averages[i], bins=40, density=True, color=\"tab:blue\", alpha=0.5, label=\"train $\\\\bar z$\")\n",
    "            # plt.hist(control_k_averages, bins=40, density=True, color=\"tab:orange\", alpha=0.5, label=\"hindcast $\\\\bar z$\")\n",
    "            plt.hist(test_k_averages[i], bins=40, density=True, color=\"tab:orange\", alpha=0.5, label=\"covid $\\\\bar z$\")\n",
    "            plt.title(f\"{stations[i]}, k={k}\")\n",
    "            plt.xlabel(\"sample mean $\\zeta$\")\n",
    "            l = np.linspace(min(train_k_averages[i]), max(train_k_averages[i]))\n",
    "            # if consecutive z-scores were iid, we would expect this distribution to have std = 1/sqrt(k)\n",
    "            plt.plot(l, norm.pdf(l, loc=np.mean(train_k_averages[i]), scale=np.std(train_k_averages[i])), label=\"normal fit\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    good_station_idxs = list(range(len(stations)))\n",
    "    good_stations = stations[good_station_idxs]\n",
    "    # run PCA on these, after time aligning them\n",
    "    all_train_ts = np.unique(np.concatenate(np.array(train_ts, dtype=object)[good_station_idxs])).astype(np.float64)\n",
    "    all_test_ts = np.unique(np.concatenate(np.array(test_ts, dtype=object)[good_station_idxs])).astype(np.float64)\n",
    "\n",
    "    train_zeta_avg_mat = np.full((len(all_train_ts), len(stations)), fill_value=np.nan)  # [t, dim]\n",
    "    test_zeta_avg_mat = np.full((len(all_test_ts), len(stations)), fill_value=np.nan)  # [t, dim]\n",
    "\n",
    "    for i, station in zip(good_station_idxs, good_stations):\n",
    "        # TODO deal with train_k_averages being empty. You can probably just pass on error\n",
    "        interp = interpolate.interp1d(train_ts[i], train_k_averages[i], bounds_error=False, fill_value=np.nanmean(train_k_averages[i]))\n",
    "        train_zeta_avg_mat[:, i] = interp(all_train_ts)\n",
    "\n",
    "        interp = interpolate.interp1d(test_ts[i], test_k_averages[i], bounds_error=False, fill_value=np.nanmean(test_k_averages[i]))\n",
    "        test_zeta_avg_mat[:, i] = interp(all_test_ts)\n",
    "\n",
    "    train_zeta_avg_mat = train_zeta_avg_mat[:, good_station_idxs]\n",
    "    test_zeta_avg_mat = test_zeta_avg_mat[:, good_station_idxs]\n",
    "    return all_train_ts, all_test_ts, train_zeta_avg_mat, test_zeta_avg_mat, train_end_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nd_cdf(n, radial_density=norm):\n",
    "    \"\"\"returns a function that, given a radius r, returns the probability\n",
    "    of an observation being as close or closer to the origin in n dimensional\n",
    "    space than r, assuming observations are drawn from a distribution centered\n",
    "    at the origin and all of whose radial cross sections have density function `radial_density`\"\"\"\n",
    "    stop = 50\n",
    "    num = 1000\n",
    "    while True:\n",
    "        radii = np.linspace(0, stop, num)\n",
    "        cdf = np.zeros_like(radii)\n",
    "        for i, r in enumerate((radii[1:] + radii[:-1]) / 2):\n",
    "            dr = radii[i + 1] - radii[i]\n",
    "            shell = radial_density.pdf(r) * r**(n - 1)\n",
    "            cdf[i + 1] = shell * dr + cdf[i]\n",
    "        cdf /= max(cdf)\n",
    "\n",
    "        radial_pdf = cdf[1:] - cdf[:-1]\n",
    "        if radial_pdf[-1] < 0.001 * max(radial_pdf):\n",
    "            break\n",
    "        else:\n",
    "            stop *= 2\n",
    "            num *= 2\n",
    "    return interpolate.interp1d(radii, cdf, bounds_error=True)\n",
    "\n",
    "def l2_norm(vec, axis=None):\n",
    "        return np.sqrt(np.sum(vec**2, axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mahalanobis_dist(all_train_ts, all_test_ts, train_zeta_avg_mat, test_zeta_avg_mat, train_end_t, explained_var_thresh=0.8, show=False):\n",
    "    pca = PCA()\n",
    "    pca.fit(train_zeta_avg_mat)\n",
    "    cumul_exp_var = [0] + [sum(pca.explained_variance_ratio_[:i + 1]) for i in range(len(pca.explained_variance_ratio_))]\n",
    "    if show:\n",
    "        plt.figure(dpi=500, figsize=(4, 4))\n",
    "        plt.plot(cumul_exp_var, marker=\".\")\n",
    "        plt.xlabel(\"rank\")\n",
    "        plt.ylabel(\"explained variance\")\n",
    "        plt.show()\n",
    "    train_proj = pca.transform(train_zeta_avg_mat)\n",
    "    test_proj = pca.transform(test_zeta_avg_mat)\n",
    "    if show:\n",
    "        plt.figure(dpi=500)\n",
    "        plt.scatter(train_proj[:, 0], train_proj[:, 1], s=1, c=all_train_ts)\n",
    "        axes = plt.gca()\n",
    "        axes.set_aspect(\"equal\")\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(\"training data projection\")\n",
    "        plt.figure(dpi=500, figsize=(11, 11))\n",
    "        plt.show()\n",
    "    # norm_means = [] this is just 0\n",
    "    norm_stds = []\n",
    "    for i in range(train_proj.shape[1]):\n",
    "        norm_stds.append(np.std(train_proj[:, i]))\n",
    "\n",
    "    if show:\n",
    "        plt.figure(dpi=500, figsize=(8, 20))\n",
    "        for i in range(min(train_proj.shape[1], 9)):\n",
    "            plt.subplot(3, 3, i+1)\n",
    "            # norm_means.append(np.mean(train_proj[:, i]))\n",
    "            norm_crit_l = norm.ppf(alpha, loc=0, scale=norm_stds[i])\n",
    "            norm_crit_u = norm.ppf(1-alpha, loc=0, scale=norm_stds[i])\n",
    "            plt.hist(train_proj[:, i], bins=40, density=True, color=\"tab:blue\", alpha=0.5, label=\"train $\\\\bar z$\")\n",
    "            # plt.hist(control_k_averages, bins=40, density=True, color=\"tab:orange\", alpha=0.5, label=\"hindcast $\\\\bar z$\")\n",
    "            plt.hist(test_proj[:, i], bins=40, density=True, color=\"tab:orange\", alpha=0.5, label=\"covid $\\\\bar z$\")\n",
    "            plt.axvline(norm_crit_l, color=\"gray\", label=\"$\\\\bar z^*$\")\n",
    "            plt.axvline(norm_crit_u, color=\"gray\")\n",
    "            plt.title(f\"cluster {cluster_id}, PC {i + 1}, k={k}\")\n",
    "            plt.xlabel(\"sample mean zeta-score\")\n",
    "            l = np.linspace(min(train_proj[:, i]), max(train_proj[:, i]))\n",
    "            # if consecutive z-scores were iid, we would expect this distribution to have std = 1/sqrt(k)\n",
    "            plt.plot(l, norm.pdf(l, loc=0, scale=norm_stds[i]), label=\"normal fit\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(dpi=500, figsize=(11, 22))\n",
    "        for i in range(train_proj.shape[1]):    \n",
    "            plt.subplot(6, 3, i+1)\n",
    "            plt.plot(all_train_ts, train_proj[:, i], label=\"train  $\\\\bar z$\", color=\"tab:blue\")\n",
    "            plt.plot(all_test_ts, test_proj[:, i], label=\"test  $\\\\bar z$\", color=\"tab:orange\")\n",
    "            plt.ylim([-4, 4])\n",
    "            plt.axvline(train_end_t, color=\"gray\")\n",
    "            plt.ylabel(\"$\\\\bar \\zeta$\")\n",
    "            plt.xlabel(\"t\")\n",
    "            \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    mean = np.mean(train_zeta_avg_mat, axis=0)\n",
    "    err = train_zeta_avg_mat - mean\n",
    "    cov = err.T @ err / (err.shape[0] - 1)\n",
    "    icov = np.linalg.inv(cov)\n",
    "    train_mahalanobis_dist = np.empty(len(train_zeta_avg_mat))\n",
    "    for i in range(len(train_zeta_avg_mat)):\n",
    "        train_mahalanobis_dist[i] = mahalanobis(train_zeta_avg_mat[i, :], mean, icov)\n",
    "    test_mahalanobis_dist = np.empty(len(test_zeta_avg_mat))\n",
    "    for i in range(len(test_zeta_avg_mat)):\n",
    "        test_mahalanobis_dist[i] = mahalanobis(test_zeta_avg_mat[i, :], mean, icov)\n",
    "\n",
    "    num_PCs = np.nonzero(np.array(cumul_exp_var) >= explained_var_thresh)[0].min()\n",
    "    train_mahalanobis_dist = l2_norm(np.array([train_proj[:, i] / norm_stds[i] for i in range(num_PCs)]), axis=0)\n",
    "    test_mahalanobis_dist = l2_norm(np.array([test_proj[:, i] / norm_stds[i] for i in range(num_PCs)]), axis=0)\n",
    "    print(num_PCs, \"PCs\")\n",
    "    return train_mahalanobis_dist, test_mahalanobis_dist, num_PCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lockdown_dates =  {\"birmingham\": dt.datetime(2020, 3, 20), \"manchester\": dt.datetime(2020, 3, 20), \"london\": dt.datetime(2020, 3, 20),\n",
    "                    \"losangeles\": dt.datetime(2020, 3, 16), \"seattle\": dt.datetime(2020, 3, 16), \"wuhan\": dt.datetime(2020, 1, 23),\n",
    "                    \"paris\": dt.datetime(2020, 3, 17), \"milan\": dt.datetime(2020, 3, 9),\n",
    "                    \"madrid\": dt.datetime(2020, 3, 13), \n",
    "                    \"berlin\": dt.datetime(2020, 3, 19), \"barcelona\": dt.datetime(2020, 3, 13), \"santiago\": dt.datetime(2020, 3, 26),\n",
    "                    \"tokyo\": dt.datetime(2020, 4, 10), # this is very fuzzy\n",
    "                    \"hongkong\": dt.datetime(2020, 1, 25), \"teipei\": dt.datetime(2021, 5, 15), \"bangkok\": dt.datetime(2020, 3, 21),\n",
    "                    \"jerusalem\": dt.datetime(2020, 3, 15), \"newyork\": dt.datetime(2020, 3, 22)}  # beijing and shanghai are unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = \"shanghai\"\n",
    "alpha=0.001\n",
    "k=168\n",
    "explained_var_thresh = 0.9\n",
    "quality_thresh=0.5\n",
    "stations = get_stations(cluster_id, quality_thresh=quality_thresh)\n",
    "all_train_ts, all_test_ts, train_zeta_avg_mat, test_zeta_avg_mat, train_end_t = get_zeta_series(stations, k=k, show=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical anomaly detection figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "k=168\n",
    "quality_thresh=0.5\n",
    "explained_var_thresh=0.9\n",
    "\n",
    "region_id = \"madrid-barcelona\"\n",
    "city_ids = [\"madrid\", \"barcelona\"]\n",
    "all_stations = list(get_stations(region_id, quality_thresh=quality_thresh))\n",
    "regions = [region_id] + city_ids + all_stations\n",
    "region_stations = {station: [station] for station in all_stations}\n",
    "region_stations[region_id] = list(get_stations(region_id, quality_thresh=quality_thresh))\n",
    "for cid in city_ids:\n",
    "    region_stations[cid] = list(get_stations(cid, quality_thresh=quality_thresh))\n",
    "\n",
    "region_train_ts, region_test_ts, region_train_zeta_avg_mat, region_test_zeta_avg_mat, region_train_probs, region_test_probs = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "for region in region_stations:\n",
    "    stations = np.array(region_stations[region], dtype='object')\n",
    "\n",
    "    region_train_ts[region], region_test_ts[region], region_train_zeta_avg_mat[region], region_test_zeta_avg_mat[region], train_end_t = get_zeta_series(stations, k=k, show=False)\n",
    "    train_mahalanobis_dist, test_mahalanobis_dist, num_PCs = get_mahalanobis_dist(region_train_ts[region], region_test_ts[region],\n",
    "            region_train_zeta_avg_mat[region], region_test_zeta_avg_mat[region], train_end_t, explained_var_thresh=explained_var_thresh, show=False)\n",
    "    cdf = get_nd_cdf(n=num_PCs, radial_density=norm)\n",
    "    region_train_probs[region], region_test_probs[region] = (1 - cdf(abs(train_mahalanobis_dist))), (1 - cdf(abs(test_mahalanobis_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "colors = list(\"bgrcmyk\") + list(mcolors.TABLEAU_COLORS.keys())\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.figure(dpi=500, figsize=(12, 3))\n",
    "t_scale = 24 * 60 * 60\n",
    "lt = time.mktime(lockdown_dates[\"barcelona\"].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "plt.fill_between([lt / t_scale, lt / t_scale + 100], [0, 0], [11, 11], color=\"y\", alpha=0.2, label=\"lockdown\")\n",
    "plt.fill_between([0, 365 * 2], [0, 0], [11, 11], color=\"c\", alpha=0.2, label=\"train\")\n",
    "plt.fill_between([365 * 2, lt / t_scale], [0, 0], [11, 11], color=\"m\", alpha=0.2, label=\"pre-lockdown\")\n",
    "i=0\n",
    "for region in list(np.random.choice(all_stations, 20, replace=False)) + city_ids:\n",
    "    plt.plot(region_train_ts[region] / t_scale, region_train_probs[region], linewidth=1, color=\"grey\", alpha=0.35)\n",
    "    plt.plot(region_test_ts[region] / t_scale, region_test_probs[region], linewidth=1, color=\"grey\", alpha=0.35)\n",
    "    i = (i + 1) % len(colors)\n",
    "for region in [region_id]:\n",
    "    plt.plot(region_train_ts[region] / t_scale, region_train_probs[region], linewidth=2, color=\"b\", label=\"Spain\")\n",
    "    plt.plot(region_test_ts[region] / t_scale, region_test_probs[region], linewidth=2, color=\"b\")\n",
    "    i = (i + 1) % len(colors)\n",
    "plt.xlim([-20, 870])\n",
    "plt.semilogy()\n",
    "plt.gca().set_ylim(top=9)\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=[0, 0], ncol=2)\n",
    "plt.xlabel(\"days since Jan 1 2018\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.axhline(alpha, color=\"gray\", linestyle=\"--\", label=\"$\\\\alpha$\")\n",
    "plt.savefig(f\"hierarchy_{region_id}.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=100, figsize=(18, 4))\n",
    "n_bins = 30\n",
    "bins = np.array([0] + [10.0**i for i in np.linspace(-12, 0, n_bins)])\n",
    "# bins = np.linspace(0, 1, n_bins + 1)\n",
    "titles = [\"Spain\", \"Madrid\", \"Barcelona\"]\n",
    "for i, region in enumerate([region_id] + city_ids): # + list(np.random.choice(all_stations, 3, replace=False))):\n",
    "    start_idx = np.nonzero(region_test_ts[region] >= lt)[0].min()\n",
    "    end_idx = np.nonzero(region_test_ts[region] >= lt + 80 * t_scale)[0].min()\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(titles[i])\n",
    "    prelock, lock = region_test_probs[region][:start_idx - k], region_test_probs[region][start_idx:end_idx]\n",
    "    plt.hist(prelock, label=\"pre-lockdown\", color=\"m\", alpha=0.4, bins=bins, density=True) #, bins=[0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    plt.hist(lock, label=\"lockdown\", color=\"y\", alpha=0.4, bins=bins, density=True) #, bins=[0, 0.001, 0.01, 0.1, 0.5, 1]\n",
    "    print(region, np.all(lock > alpha))\n",
    "    print(region, np.any(prelock < alpha))\n",
    "    # plt.ylim([0, 10])\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title(\"individual station (avg)\")\n",
    "pre_lockdown_counts = np.zeros(n_bins)\n",
    "lockdown_counts = np.zeros(n_bins)\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "for i, region in enumerate(all_stations):\n",
    "    start_idx = np.nonzero(region_test_ts[region] >= lt)[0].min()\n",
    "    end_idx = np.nonzero(region_test_ts[region] >= lt + 80 * t_scale)[0].min()\n",
    "    prelock, lock = region_test_probs[region][:start_idx - k], region_test_probs[region][start_idx:end_idx]\n",
    "    pre_lockdown_counts += np.histogram(prelock, bins=bins, density=True)[0]\n",
    "    lockdown_counts += np.histogram(lock, bins=bins, density=True)[0]\n",
    "    false_neg += np.all(lock > alpha)\n",
    "    false_pos += np.any(prelock < alpha)\n",
    "    # plt.ylim([0, 10])\n",
    "    plt.xlim([10**-20, 1])\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "pre_lockdown_counts /= len(all_stations)\n",
    "lockdown_counts /= len(all_stations)\n",
    "plt.bar(bins[:-1], pre_lockdown_counts, align=\"edge\", label=\"pre-lockdown\", color=\"m\", alpha=0.4, width=bins[1:] - bins[:-1])\n",
    "plt.bar(bins[:-1], lockdown_counts, align=\"edge\", label=\"lockdown\", color=\"y\", alpha=0.4, width=bins[1:] - bins[:-1])\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.ylabel(\"probability density\")\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"hierarchy_hists_{region_id}.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual station DPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station =  \"EEA_IT_IT1251A\" # \"Station0004349\" \"Station0004701\" \"Station0004349\"  # Seattle  \"Station0001766\" # LA\n",
    "data_name = f\"obs_{station}\"\n",
    "for fname in os.listdir(\"forecasts\"):\n",
    "    if data_name in fname and fname.startswith(\"koop\"):\n",
    "        param_str = fname[5:-4]\n",
    "        print(fname)\n",
    "\n",
    "obs_koop = pickle.load(open(f\"forecasts/koop_{param_str}.pkl\", 'rb'))\n",
    "obs_x = np.load(f\"forecasts/x_{param_str}.npy\")\n",
    "obs_t = np.load(f\"forecasts/t_{param_str}.npy\")\n",
    "\n",
    "train_start_date = dt.datetime(2018, 1, 1) # dt.datetime(2018, 3, 16)\n",
    "train_end_date = dt.datetime(2020, 1, 1)  # covid start date dt.datetime(2020, 3, 16) for Seattle and LA, 1/23 for Wuhan, and 3/9 in Italy\n",
    "test_end_date = dt.datetime(2021, 1, 1) # dt.datetime(2020, 5, 16)\n",
    "t_min = time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "train_start_t = time.mktime(train_start_date.timetuple()) - t_min\n",
    "train_end_t = time.mktime(train_end_date.timetuple()) - t_min\n",
    "test_end_t = time.mktime(test_end_date.timetuple()) - t_min\n",
    "\n",
    "train_start = np.nonzero(obs_t >= train_start_t)[0].min()\n",
    "train_end = np.nonzero(obs_t >= train_end_t)[0].min()\n",
    "test_end = np.nonzero(obs_t <= test_end_t)[0].max()\n",
    "\n",
    "params = obs_koop.predict(obs_t, covariates=obs_t.reshape(len(obs_t), 1))\n",
    "mean_hat = obs_koop.model_obj.mean(params)\n",
    "std_hat = obs_koop.model_obj.std(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "CIs = dict()\n",
    "percentiles = [0.1, 0.2]\n",
    "for percentile in percentiles:\n",
    "    CIs[percentile] = (norm.ppf(percentile, loc=mean_hat.flatten(), scale=std_hat.flatten()), norm.ppf(1 - percentile, loc=mean_hat.flatten(), scale=std_hat.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=500, figsize=(7.5, 2.5))\n",
    "t_scale = 60 * 60 * 24\n",
    "plt.plot(obs_t / t_scale - 730, obs_x, c=\"k\", linewidth=0.5, label=\"observation\")\n",
    "for i, percentile in enumerate(percentiles):\n",
    "    lo, hi = CIs[percentile]\n",
    "    label = None if i != 0 else \"DPK\"\n",
    "    plt.fill_between(obs_t / t_scale - 730, lo, hi, alpha=0.2, facecolor=\"b\", linewidth=0, label=label)\n",
    "plt.xlabel(\"days since Jan 1 2020\")\n",
    "plt.ylabel(\"$\\log[$NO$_2]$\")\n",
    "plt.xlim([45, 100])\n",
    "plt.ylim([1, 7])\n",
    "plt.fill_between([68, 200], [0, 0], [10, 10], color=\"yellow\", alpha=0.2, label=\"lockdown\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"madrid_DPK.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sweep over k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "# F_\\beta \"measures the effectiveness of retrieval with respect to a user who attaches \\beta times as much importance to recall as precision\"\n",
    "# If the class ratio was 50/50 I would assign ~2x as much weight to recall, but since the class ratio is really more like 99/1 I don't want to be spammed with false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lockdown_dates =  {\"birmingham\": dt.datetime(2020, 3, 20), \"manchester\": dt.datetime(2020, 3, 20), \"london\": dt.datetime(2020, 3, 20),\n",
    "                    \"losangeles\": dt.datetime(2020, 3, 16), \"seattle\": dt.datetime(2020, 3, 16), \"wuhan\": dt.datetime(2020, 1, 23),\n",
    "                    \"paris\": dt.datetime(2020, 3, 17), \"milan\": dt.datetime(2020, 3, 9),\n",
    "                    \"madrid\": dt.datetime(2020, 3, 13), \n",
    "                    \"berlin\": dt.datetime(2020, 3, 19), \"barcelona\": dt.datetime(2020, 3, 13), \"santiago\": dt.datetime(2020, 3, 26),\n",
    "                    \"beijing\": dt.datetime(2020, 1, 1), \"shanghai\": dt.datetime(2020, 1, 2), \"tokyo\": dt.datetime(2020, 4, 10), # this is very fuzzy\n",
    "                    \"hongkong\": dt.datetime(2020, 1, 25), \"teipei\": dt.datetime(2021, 5, 15), \"bangkok\": dt.datetime(2020, 3, 21),\n",
    "                    \"jerusalem\": dt.datetime(2020, 3, 15), \"newyork\": dt.datetime(2020, 3, 22)}  # beijing and shanghai are unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_thresh = 0.5\n",
    "beta = 1/6  # I want 6x more precision than recall\n",
    "n_regions = len(lockdown_dates)\n",
    "sweep_regions = np.random.choice(list(lockdown_dates), n_regions, replace=False)\n",
    "sweep_stations = {region: get_stations(region, quality_thresh=quality_thresh) for region in sweep_regions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "k_values = [1, 6, 24, 48, 72, 120, 168, 336]\n",
    "explained_var_thresh_values = [0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 0.999]\n",
    "lag_mats, F1_mats = {r: np.full((len(explained_var_thresh_values), len(k_values), len(alphas)), np.nan) for r in sweep_regions}, {r: np.full((len(explained_var_thresh_values), len(k_values), len(alphas)), np.nan) for r in sweep_regions}\n",
    "all_train_ts, all_test_ts, all_train_zeta_avg_mat, all_test_zeta_avg_mat, all_train_probs, all_test_probs = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "for i, evt in tqdm(list(enumerate(explained_var_thresh_values)), total=len(explained_var_thresh_values)):\n",
    "    all_train_ts[evt], all_test_ts[evt], all_train_zeta_avg_mat[evt], all_test_zeta_avg_mat[evt], all_train_probs[evt], all_test_probs[evt] = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "    for j, k in enumerate(k_values):\n",
    "        all_train_ts[evt][k], all_test_ts[evt][k], all_train_zeta_avg_mat[evt][k], all_test_zeta_avg_mat[evt][k], all_train_probs[evt][k], all_test_probs[evt][k] = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "        for region in sweep_regions:\n",
    "            try:\n",
    "                stations = np.array(sweep_stations[region], dtype='object')\n",
    "\n",
    "                all_train_ts[evt][k][region], all_test_ts[evt][k][region], all_train_zeta_avg_mat[evt][k][region], all_test_zeta_avg_mat[evt][k][region], all_train_end_t = get_zeta_series(stations, k=k, show=False)\n",
    "                train_mahalanobis_dist, test_mahalanobis_dist, num_PCs = get_mahalanobis_dist(all_train_ts[evt][k][region], all_test_ts[evt][k][region],\n",
    "                        all_train_zeta_avg_mat[evt][k][region], all_test_zeta_avg_mat[evt][k][region], train_end_t, explained_var_thresh=evt, show=False)\n",
    "                cdf = get_nd_cdf(n=num_PCs, radial_density=norm)\n",
    "                all_train_probs[evt][k][region], all_test_probs[evt][k][region] = (1 - cdf(abs(train_mahalanobis_dist))), (1 - cdf(abs(test_mahalanobis_dist)))\n",
    "\n",
    "                lt = time.mktime(lockdown_dates[region].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "                tt = all_test_ts[evt][k][region]\n",
    "                control = all_test_probs[evt][k][region][tt < lt]\n",
    "                test = all_test_probs[evt][k][region][(lt + k <= tt) & (tt < lt + 50 * 24 * 60 * 60)]\n",
    "                for l, alpha in enumerate(alphas):\n",
    "                    is_anomaly_control = control < alpha\n",
    "                    is_anomaly_test = test < alpha\n",
    "                    lab = list(np.full(len(control), False)) + list(np.full(len(test), True))\n",
    "                    F1_mats[region][i, j, l] = fbeta_score(lab, list(is_anomaly_control) + list(is_anomaly_test), beta=beta, pos_label=True, average=\"binary\")\n",
    "                    anomaly_ts = all_test_ts[evt][k][region][(lt + k <= tt) & (tt < lt + 50 * 24 * 60 * 60) & (all_test_probs[evt][k][region] < alpha)]\n",
    "                    lag_mats[region][i, j, l] = anomaly_ts[0] - lt if len(anomaly_ts) > 0 else np.inf\n",
    "            except Exception as e:\n",
    "                print(\"\\n\\n\\nException:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"all_train_ts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_train_ts), f)\n",
    "with open(\"all_test_ts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_test_ts), f)\n",
    "with open(\"all_train_zeta_avg_mat.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_train_zeta_avg_mat), f)\n",
    "with open(\"all_test_zeta_avg_mat.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_test_zeta_avg_mat), f)\n",
    "with open(\"all_train_probs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_train_probs), f)\n",
    "with open(\"all_test_probs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(all_test_probs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F1_mats.json\", \"w\") as f:\n",
    "    f.write(json.dumps({m: F1_mats[m].tolist() for m in F1_mats}))\n",
    "with open(\"lag_mats.json\", \"w\") as f:\n",
    "    f.write(json.dumps({m: lag_mats[m].tolist() for m in lag_mats}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in F1_mats if not np.any(np.isnan(F1_mats[r]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consider_regions = {\"milan\", \"hongkong\", \"barcelona\", \"wuhan\", \"manchester\", \"madrid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show zeta vs time for each k x explained var in one plot. \n",
    "# Show latency vs k vs explained var. \n",
    "# Show F1 score (ignoring the k timesteps at the boundary) vs k and explained var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_F1s = np.nanmean([F1_mats[r] for r in consider_regions], axis=0)\n",
    "plt.figure(dpi=1000, figsize=(7 * 3, 7 * 2))\n",
    "plt.suptitle(\"$F_{1/6}$ score\")\n",
    "for a_idx in range(6):\n",
    "    plt.subplot(2, 3, a_idx+1)\n",
    "    plt.title(f\"alpha={alphas[a_idx]}\")\n",
    "    plt.imshow(avg_F1s[:,:,a_idx], origin=\"lower\", cmap=\"viridis\", vmin=0, vmax=1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(len(k_values)), labels=k_values)\n",
    "    ax.set_yticks(np.arange(len(explained_var_thresh_values)), labels=[round(v, 2) for v in explained_var_thresh_values])\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    # plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "    #          rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(explained_var_thresh_values)):\n",
    "        for j in range(len(k_values)):\n",
    "            text = ax.text(j, i, round(avg_F1s[i, j, a_idx], 2),\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", weight=\"bold\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.xlabel(\"k\")\n",
    "plt.tight_layout()\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in lag_mats.values():\n",
    "    m[np.isnan(m) | (m == np.inf) | (m == 100) | (m == 100 * t_scale)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scale = 24 * 60 * 60\n",
    "avg_lags = np.nanmean([lag_mats[r] for r in consider_regions], axis=0) / t_scale\n",
    "plt.figure(dpi=1000, figsize=(7 * 3, 7 * 2))\n",
    "plt.suptitle(\"latency (days from lockdown), conditioned on detection\")\n",
    "for a_idx in range(6):\n",
    "    plt.subplot(2, 3, a_idx+1)\n",
    "    plt.title(f\"alpha={alphas[a_idx]}\")\n",
    "    plt.imshow(avg_lags[:,:,a_idx], cmap=\"viridis_r\", origin=\"lower\", vmin=0, vmax=20)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(len(k_values)), labels=k_values)\n",
    "    ax.set_yticks(np.arange(len(explained_var_thresh_values)), labels=[round(v, 2) for v in explained_var_thresh_values])\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    # plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "    #          rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(explained_var_thresh_values)):\n",
    "        for j in range(len(k_values)):\n",
    "            text = ax.text(j, i, round(avg_lags[i, j, a_idx], 1),\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", weight=\"bold\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.xlabel(\"k\")\n",
    "plt.tight_layout()\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration for multidim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_test_ts.pkl\", \"rb\") as f:\n",
    "    all_test_ts = pickle.load(f).to_dict()\n",
    "with open(\"all_test_probs.pkl\", \"rb\") as f:\n",
    "    all_test_probs = pickle.load(f).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lockdown_dates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seattle had an unusual snowstorm keeping people inside early jan (https://www.seattlepi.com/local/weather/article/Seattle-history-cold-arctic-ice-snow-storm-1950-14974927.php)\n",
    "# madrid, barcelona has storm gloria\n",
    "# tokyo, jerusalem, paris, losangeles, santiago not in all_test... (Exception)\n",
    "# Birmingham, London, Berlin storm ciara https://www.trtworld.com/europe/uk-faces-another-fierce-storm-2-found-dead-in-rough-seas-33827, https://en.wikipedia.org/wiki/Storm_Ciara\n",
    "consider_regions = {\"milan\", \"hongkong\", \"newyork\", \"teipei\", \"manchester\"}  \n",
    "consider_test_ts = dict((r, all_test_ts[0.9][168][r]) for r in consider_regions)\n",
    "consider_test_probs = dict((r, all_test_probs[0.9][168][r]) for r in consider_regions)\n",
    "for region in consider_regions:\n",
    "    lt = time.mktime(lockdown_dates[region].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "    print(lt)\n",
    "    idx = np.nonzero(consider_test_ts[region] < lt)[0].max()\n",
    "    consider_test_probs[region] = consider_test_probs[region][:idx]\n",
    "    consider_test_ts[region] = consider_test_ts[region][:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 16\n",
    "# bins = np.linspace(0, 1, n_bins + 1)\n",
    "bins = 10**-np.linspace(0, 6, n_bins+1)[::-1]\n",
    "pre_lockdown_counts = np.zeros(n_bins)\n",
    "false_pos = 0\n",
    "plt.figure(figsize=(8, 3))\n",
    "n_samples = 0\n",
    "for i, region in enumerate(consider_regions):\n",
    "    pre_lockdown_counts += np.histogram(consider_test_probs[region], bins=bins, density=False)[0]\n",
    "    false_pos += np.any(consider_test_probs[region] < alpha)\n",
    "    n_samples += len(consider_test_probs[region])\n",
    "    # plt.ylim([0, 10])\n",
    "pre_lockdown_counts /= (bins[1:] - bins[:-1]) * n_samples\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlim([10**-5, 1])\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "plt.bar(bins[:-1], pre_lockdown_counts, align=\"edge\", label=\"pre-lockdown\", color=\"m\", alpha=0.4, width=bins[1:] - bins[:-1])\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1])\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.ylabel(\"probability density\")\n",
    "plt.subplot(1, 2, 2)\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "pre_lockdown_counts = np.zeros(n_bins)\n",
    "n_samples = 0\n",
    "for i, region in enumerate(consider_regions):\n",
    "    pre_lockdown_counts += np.histogram(consider_test_probs[region], bins=bins, density=False)[0]\n",
    "    false_pos += np.any(consider_test_probs[region] < alpha)\n",
    "    n_samples += len(consider_test_probs[region])\n",
    "    # plt.ylim([0, 10])\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "pre_lockdown_counts *= n_bins / n_samples\n",
    "plt.bar(bins[:-1], pre_lockdown_counts, align=\"edge\", label=\"pre-lockdown\", color=\"m\", alpha=0.4, width=bins[1:] - bins[:-1])\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.ylabel(\"probability density\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"validation_hist.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for anomalies accounted for by the model in an individual station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import time\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "station_names = {'Station0004436',}  # {'Station0004349', 'Station0004436', 'Station0004701'}\n",
    "explained_var_thresh = 0.8\n",
    "k = 168\n",
    "lamb = 2\n",
    "alpha = 0.001\n",
    "\n",
    "train_start_date = dt.datetime(2018, 1, 1)\n",
    "train_end_date = dt.datetime(2020, 1, 1)\n",
    "test_end_date = dt.datetime(2021, 1, 1)\n",
    "t_min = time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "train_start_t = time.mktime(train_start_date.timetuple()) - t_min\n",
    "train_end_t = time.mktime(train_end_date.timetuple()) - t_min\n",
    "test_end_t = time.mktime(test_end_date.timetuple()) - t_min\n",
    "\n",
    "plt.figure(dpi=1000, figsize=(14, 9))\n",
    "scale_t = 60 * 60 * 24\n",
    "offset_t = 973\n",
    "for i, station in enumerate(station_names):\n",
    "    stations = [station]\n",
    "\n",
    "    # load the koopman model and relevant data for station\n",
    "    obs_t, obs_z_scores = load_z_scores(f\"obs_{station}\")\n",
    "\n",
    "    # model_analys\n",
    "    mod_t, mod_z_scores = load_z_scores(f\"model_forecast_{station}\")\n",
    "\n",
    "    train_start = np.nonzero(obs_t >= train_start_t)[0].min()\n",
    "    train_end = np.nonzero(obs_t >= train_end_t)[0].min()\n",
    "    test_end = np.nonzero(obs_t <= test_end_t)[0].max()\n",
    "\n",
    "    f = interpolate.interp1d(mod_t, mod_z_scores)\n",
    "    aligned_mod_z_scores = f(obs_t)\n",
    "    zeta_scores = obs_z_scores - aligned_mod_z_scores\n",
    "    train_k_averages = get_contiguous_k_samples(k, zeta_scores[train_start:train_end]).mean(axis=1)\n",
    "    obs_train_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_start:train_end]).mean(axis=1)\n",
    "    mod_train_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_start:train_end]).mean(axis=1)\n",
    "    train_ts = get_contiguous_k_samples(k, obs_t[train_start:train_end]).max(axis=1)  # represents the \"real\" time\n",
    "    test_k_averages = get_contiguous_k_samples(k, zeta_scores[train_end:test_end]).mean(axis=1)\n",
    "    obs_test_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_end:test_end]).mean(axis=1)\n",
    "    mod_test_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_end:test_end]).mean(axis=1)\n",
    "    test_ts = get_contiguous_k_samples(k, obs_t[train_end:test_end]).max(axis=1)\n",
    "\n",
    "    # remove outliers from training data\n",
    "    zeta_train_ts, train_k_averages = remove_outliers(train_ts, train_k_averages, lamb=lamb)\n",
    "    obs_train_ts, obs_train_k_averages = remove_outliers(train_ts, obs_train_k_averages, lamb=lamb)\n",
    "    mod_train_ts, mod_train_k_averages = remove_outliers(train_ts, mod_train_k_averages, lamb=lamb)\n",
    "\n",
    "    of_factor = 1.4 # [1, 1.4, 1.3][i]  # adjust for overfitting\n",
    "    zeta_norm = norm(train_k_averages.mean(), of_factor * train_k_averages.std())\n",
    "    zeta_cdf = get_nd_cdf(n=1, radial_density=zeta_norm)\n",
    "    obs_cdf = get_nd_cdf(n=1, radial_density=norm(obs_train_k_averages.mean(), of_factor * obs_train_k_averages.std()))\n",
    "    mod_cdf = get_nd_cdf(n=1, radial_density=norm(mod_train_k_averages.mean(), of_factor * mod_train_k_averages.std()))\n",
    "    train_probs, test_probs = (1 - zeta_cdf(abs(train_k_averages))), (1 - zeta_cdf(abs(test_k_averages)))\n",
    "    zeta_crit_l, zeta_crit_u = zeta_norm.ppf(alpha), zeta_norm.mean() + (zeta_norm.mean() - zeta_norm.ppf(alpha))\n",
    "    obs_train_probs, obs_test_probs = (1 - obs_cdf(abs(obs_train_k_averages))), (1 - obs_cdf(abs(obs_test_k_averages)))\n",
    "    mod_train_probs, mod_test_probs = (1 - mod_cdf(abs(mod_train_k_averages))), (1 - mod_cdf(abs(mod_test_k_averages)))\n",
    "\n",
    "    typeI_mask = (test_probs > alpha) & (obs_test_probs < alpha)\n",
    "    if i < 9:\n",
    "        plt.subplot(2, 1, 2*i + 1)\n",
    "        plt.title(\"Seattle: \" + station)\n",
    "        lt = time.mktime(dt.datetime(2020, 9, 8).timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "        plt.fill_between([lt / scale_t - offset_t, lt / scale_t - offset_t + 35], [-11, -11], [11, 11], color=\"y\", alpha=0.2, label=\"wildfires\")\n",
    "        plt.plot(test_ts / scale_t - offset_t, test_k_averages, color=\"c\", linewidth=4, label=\"$\\\\bar \\zeta$\")\n",
    "        plt.plot(test_ts / scale_t - offset_t, obs_test_k_averages, color=\"m\", label=\"$\\\\bar z_{obs}$\")\n",
    "        plt.plot(test_ts / scale_t - offset_t, mod_test_k_averages, color=\"b\", label=\"$\\\\bar z_{mod}$\")\n",
    "        plt.fill_between(test_ts / scale_t - offset_t, obs_test_k_averages, mod_test_k_averages, color=\"c\",\n",
    "                         alpha=0.5, linewidth=0)\n",
    "        plt.axhline(zeta_crit_l, color=\"gray\", linestyle=\"--\")\n",
    "        plt.axhline(0, color=\"gray\", linestyle=\"-\")\n",
    "        plt.axhline(zeta_crit_u, color=\"gray\", linestyle=\"--\", label=\"$\\\\bar \\zeta^*$\")\n",
    "        plt.fill_betweenx([-3, 3], [test_ts[typeI_mask][0] / scale_t - offset_t]*2, [test_ts[typeI_mask][-1] / scale_t - offset_t]*2, color=(1., 1., 1., 0.), edgecolor=(1., 0., 0., 0.2), hatch=\"xx\", label=\"mitigated type I err\")\n",
    "        plt.gca().set_xlim(left=5, right=60)\n",
    "        plt.ylim([-2.6, 2.6])\n",
    "plt.xlabel(\"days since Sept 1 2020\")\n",
    "plt.ylabel(f\"{k}-hr mean statistic\")\n",
    "plt.legend(loc=[1.04, 0.17]) # loc=[0, -1])\n",
    "\n",
    "station_names = {'EEA_ES_ES1426A',}# {'EEA_IT_IT1251A','EEA_IT_IT1203A','EEA_IT_IT2232A'}\n",
    "explained_var_thresh = 0.8\n",
    "k = 168\n",
    "lamb = 2\n",
    "alpha = 0.001\n",
    "\n",
    "train_start_date = dt.datetime(2018, 1, 1)\n",
    "train_end_date = dt.datetime(2020, 1, 1)\n",
    "test_end_date = dt.datetime(2021, 1, 1)\n",
    "t_min = time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "train_start_t = time.mktime(train_start_date.timetuple()) - t_min\n",
    "train_end_t = time.mktime(train_end_date.timetuple()) - t_min\n",
    "test_end_t = time.mktime(test_end_date.timetuple()) - t_min\n",
    "\n",
    "scale_t = 60 * 60 * 24\n",
    "offset_t = 789\n",
    "for i, station in enumerate(station_names):\n",
    "    stations = [station]\n",
    "\n",
    "    # load the koopman model and relevant data for station\n",
    "    obs_t, obs_z_scores = load_z_scores(f\"obs_{station}\")\n",
    "\n",
    "    # model_analys\n",
    "    mod_t, mod_z_scores = load_z_scores(f\"model_forecast_{station}\")\n",
    "\n",
    "    train_start = np.nonzero(obs_t >= train_start_t)[0].min()\n",
    "    train_end = np.nonzero(obs_t >= train_end_t)[0].min()\n",
    "    test_end = np.nonzero(obs_t <= test_end_t)[0].max()\n",
    "\n",
    "    f = interpolate.interp1d(mod_t, mod_z_scores)\n",
    "    aligned_mod_z_scores = f(obs_t)\n",
    "    zeta_scores = obs_z_scores - aligned_mod_z_scores\n",
    "    train_k_averages = get_contiguous_k_samples(k, zeta_scores[train_start:train_end]).mean(axis=1)\n",
    "    obs_train_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_start:train_end]).mean(axis=1)\n",
    "    mod_train_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_start:train_end]).mean(axis=1)\n",
    "    train_ts = get_contiguous_k_samples(k, obs_t[train_start:train_end]).max(axis=1)  # represents the \"real\" time\n",
    "    test_k_averages = get_contiguous_k_samples(k, zeta_scores[train_end:test_end]).mean(axis=1)\n",
    "    obs_test_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_end:test_end]).mean(axis=1)\n",
    "    mod_test_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_end:test_end]).mean(axis=1)\n",
    "    test_ts = get_contiguous_k_samples(k, obs_t[train_end:test_end]).max(axis=1)\n",
    "\n",
    "    # remove outliers from training data\n",
    "    zeta_train_ts, train_k_averages = remove_outliers(train_ts, train_k_averages, lamb=lamb)\n",
    "    obs_train_ts, obs_train_k_averages = remove_outliers(train_ts, obs_train_k_averages, lamb=lamb)\n",
    "    mod_train_ts, mod_train_k_averages = remove_outliers(train_ts, mod_train_k_averages, lamb=lamb)\n",
    "\n",
    "    of_factor = 1\n",
    "    zeta_norm = norm(train_k_averages.mean(), of_factor * train_k_averages.std())\n",
    "    zeta_cdf = get_nd_cdf(n=1, radial_density=zeta_norm)\n",
    "    obs_cdf = get_nd_cdf(n=1, radial_density=norm(obs_train_k_averages.mean(), of_factor * obs_train_k_averages.std()))\n",
    "    mod_cdf = get_nd_cdf(n=1, radial_density=norm(mod_train_k_averages.mean(), of_factor * mod_train_k_averages.std()))\n",
    "    train_probs, test_probs = (1 - zeta_cdf(abs(train_k_averages))), (1 - zeta_cdf(abs(test_k_averages)))\n",
    "    zeta_crit_l, zeta_crit_u = zeta_norm.ppf(alpha), zeta_norm.mean() + (zeta_norm.mean() - zeta_norm.ppf(alpha))\n",
    "    obs_train_probs, obs_test_probs = (1 - obs_cdf(abs(obs_train_k_averages))), (1 - obs_cdf(abs(obs_test_k_averages)))\n",
    "    mod_train_probs, mod_test_probs = (1 - mod_cdf(abs(mod_train_k_averages))), (1 - mod_cdf(abs(mod_test_k_averages)))\n",
    "    \n",
    "    typeII_mask = (test_probs < alpha) & (obs_test_probs > alpha)\n",
    "    if i < 9:\n",
    "        plt.subplot(2, 1, 2*i + 2)\n",
    "        plt.title(\"Madrid: \" + station)\n",
    "        plt.plot(test_ts / scale_t - offset_t, test_k_averages, color=\"c\", linewidth=4)\n",
    "        plt.plot(test_ts / scale_t - offset_t, obs_test_k_averages, color=\"m\")\n",
    "        plt.plot(test_ts / scale_t - offset_t, mod_test_k_averages, color=\"b\")\n",
    "        plt.fill_between(test_ts / scale_t - offset_t, obs_test_k_averages, mod_test_k_averages, color=\"c\",\n",
    "                         alpha=0.5, linewidth=0)\n",
    "        lt = time.mktime(lockdown_dates[\"madrid\"].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "        plt.fill_between([lt / scale_t - offset_t, lt / scale_t - offset_t + 50], [-11, -11], [11, 11], color=\"y\", alpha=0.2, label=\"lockdown\")\n",
    "        plt.axhline(zeta_crit_l, color=\"gray\", linestyle=\"--\")\n",
    "        plt.axhline(0, color=\"gray\", linestyle=\"-\")\n",
    "        plt.axhline(zeta_crit_u, color=\"gray\", linestyle=\"--\")\n",
    "        plt.fill_betweenx([-3, 3], [test_ts[typeII_mask][0] / scale_t - offset_t]*2, [test_ts[typeII_mask][-1] / scale_t - offset_t]*2, color=(1., 1., 1., 0.), edgecolor=(1., 0., 0., 0.2), hatch=\"/\", label=\"mitigated type II err\")\n",
    "        plt.gca().set_xlim(left=1, right=60)\n",
    "        plt.ylim([-2.6, 2.6])\n",
    "plt.xlabel(\"days since March 1 2020\")\n",
    "plt.legend(loc=[1.04, 0.72]) # loc=[0, -1])\n",
    "plt.ylabel(f\"{k}-hr mean statistic\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"seattle_v_madrid_zeta.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z vs zeta figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('xtick', labelsize=16) \n",
    "matplotlib.rc('ytick', labelsize=16) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "station = \"EEA_ES_ES0118A\"\n",
    "explained_var_thresh = 0.8\n",
    "k = 168\n",
    "lamb = 2\n",
    "alpha = 0.001\n",
    "\n",
    "train_start_date = dt.datetime(2018, 1, 1)\n",
    "train_end_date = dt.datetime(2020, 1, 1)\n",
    "test_end_date = dt.datetime(2021, 1, 1)\n",
    "t_min = time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "train_start_t = time.mktime(train_start_date.timetuple()) - t_min\n",
    "train_end_t = time.mktime(train_end_date.timetuple()) - t_min\n",
    "test_end_t = time.mktime(test_end_date.timetuple()) - t_min\n",
    "\n",
    "scale_t = 60 * 60 * 24\n",
    "offset_t = 0\n",
    "stations = [station]\n",
    "\n",
    "# load the koopman model and relevant data for station\n",
    "obs_t, obs_z_scores = load_z_scores(f\"obs_{station}\")\n",
    "mod_t, mod_z_scores = load_z_scores(f\"model_forecast_{station}\")\n",
    "\n",
    "train_start = np.nonzero(obs_t >= train_start_t)[0].min()\n",
    "train_end = np.nonzero(obs_t >= train_end_t)[0].min()\n",
    "test_end = np.nonzero(obs_t <= test_end_t)[0].max()\n",
    "\n",
    "f = interpolate.interp1d(mod_t, mod_z_scores)\n",
    "aligned_mod_z_scores = f(obs_t)\n",
    "zeta_scores = obs_z_scores - aligned_mod_z_scores\n",
    "train_k_averages = get_contiguous_k_samples(k, zeta_scores[train_start:train_end]).mean(axis=1)\n",
    "obs_train_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_start:train_end]).mean(axis=1)\n",
    "mod_train_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_start:train_end]).mean(axis=1)\n",
    "train_ts = get_contiguous_k_samples(k, obs_t[train_start:train_end]).max(axis=1)  # represents the \"real\" time\n",
    "test_k_averages = get_contiguous_k_samples(k, zeta_scores[train_end:test_end]).mean(axis=1)\n",
    "obs_test_k_averages = get_contiguous_k_samples(k, obs_z_scores[train_end:test_end]).mean(axis=1)\n",
    "mod_test_k_averages = get_contiguous_k_samples(k, aligned_mod_z_scores[train_end:test_end]).mean(axis=1)\n",
    "test_ts = get_contiguous_k_samples(k, obs_t[train_end:test_end]).max(axis=1)\n",
    "\n",
    "# remove outliers from training data\n",
    "zeta_train_ts, zeta_train_k_averages = remove_outliers(train_ts, train_k_averages, lamb=lamb)\n",
    "obs_train_ts, obs_train_k_averages = remove_outliers(train_ts, obs_train_k_averages, lamb=lamb)\n",
    "\n",
    "of_factor = 1  # adjust for overfitting\n",
    "obs_norm = norm(obs_train_k_averages.mean(), of_factor * obs_train_k_averages.std())\n",
    "zeta_norm = norm(zeta_train_k_averages.mean(), of_factor * zeta_train_k_averages.std())\n",
    "obs_cdf = get_nd_cdf(n=1, radial_density=obs_norm)\n",
    "zeta_cdf = get_nd_cdf(n=1, radial_density=zeta_norm)\n",
    "obs_train_probs, obs_test_probs = (1 - obs_cdf(abs(obs_train_k_averages))), (1 - obs_cdf(abs(obs_test_k_averages)))\n",
    "zeta_train_probs, zeta_test_probs = (1 - zeta_cdf(abs(zeta_train_k_averages))), (1 - zeta_cdf(abs(test_k_averages)))\n",
    "z_crit = -obs_norm.ppf(alpha)\n",
    "zeta_crit = -zeta_norm.ppf(alpha)\n",
    "\n",
    "layout = \\\n",
    "\"\"\"\n",
    "AAB\n",
    "\"\"\"\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, dpi=1500, figsize=(18, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "plt.sca(ax1)\n",
    "lt = time.mktime(lockdown_dates[\"madrid\"].timetuple()) - t_min\n",
    "ax1.plot(obs_train_ts / scale_t - offset_t, obs_train_k_averages, color=\"c\", label=\"$\\\\bar z_{obs}$\")\n",
    "ax1.plot(test_ts[test_ts < lt + 60 * scale_t] / scale_t - offset_t, obs_test_k_averages[test_ts < lt + 60 * scale_t], color=\"c\")\n",
    "plt.fill_between([0, train_end_t / scale_t], [-3, -3], [3, 3], color=\"c\", alpha=0.2, label=\"train\")\n",
    "plt.fill_between([train_end_t / scale_t, lt / scale_t], [-3, -3], [3, 3], color=\"m\", alpha=0.2, label=\"pre-lockdown\")\n",
    "plt.fill_between([lt / scale_t, lt / scale_t + 60], [-3, -3], [3, 3], color=\"y\", alpha=0.2, label=\"lockdown\")\n",
    "plt.axhline(z_crit, color=\"gray\", linestyle=\"--\")\n",
    "plt.axhline(-z_crit, label=\"$\\\\bar z^*$\", color=\"gray\", linestyle=\"--\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"-\")\n",
    "ax1.set_xlabel(\"days since Jan 1 2018\")\n",
    "ax1.set_ylabel(\"$\\\\bar z_{obs}$\")\n",
    "ax1.legend(loc=[1.04, 0.36]) # loc=[0, -1])\n",
    "plt.ylim([-2.8, 2.8])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.hist(obs_test_k_averages[test_ts < lt], color=\"m\", alpha=0.4, label=\"pre-lockdown\", density=True)\n",
    "plt.hist(obs_test_k_averages[(lt < test_ts) & (test_ts < lt + 60 * scale_t)], color=\"y\", alpha=0.4, label=\"lockdown\", density=True)\n",
    "plt.hist(obs_train_k_averages, color=\"c\", alpha=0.4, label=\"train\", density=True)\n",
    "plt.axvline(z_crit, label=\"$\\\\bar z^*$\", color=\"gray\", linestyle=\"--\")\n",
    "plt.axvline(-z_crit, color=\"gray\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"gray\", linestyle=\"-\")\n",
    "ax2.set_ylabel(\"density\")\n",
    "ax2.set_xlabel(\"$\\\\bar z_{obs}$\")\n",
    "plt.ylim([0, 2.7])\n",
    "plt.xlim([-2.8, 2.8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"z_\" + station + \".pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, dpi=1500, figsize=(18, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "plt.sca(ax1)\n",
    "ax1.plot(train_ts / scale_t - offset_t, train_k_averages, color=\"c\", label=\"$\\\\bar \\zeta$\")\n",
    "ax1.plot(test_ts[test_ts < lt + 60 * scale_t] / scale_t - offset_t, test_k_averages[test_ts < lt + 60 * scale_t], color=\"c\")\n",
    "lt = time.mktime(lockdown_dates[\"madrid\"].timetuple()) - t_min\n",
    "plt.fill_between([0, train_end_t / scale_t], [-3, -3], [3, 3], color=\"c\", alpha=0.2, label=\"train\")\n",
    "plt.fill_between([train_end_t / scale_t, lt / scale_t], [-3, -3], [3, 3], color=\"m\", alpha=0.2, label=\"pre-lockdown\")\n",
    "plt.fill_between([lt / scale_t, lt / scale_t + 60], [-3, -3], [3, 3], color=\"y\", alpha=0.2, label=\"lockdown\")\n",
    "plt.axhline(zeta_crit, color=\"gray\", linestyle=\"--\")\n",
    "plt.axhline(-zeta_crit, label=\"$\\\\bar \\zeta^*$\", color=\"gray\", linestyle=\"--\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"-\")\n",
    "ax1.set_xlabel(\"days since Jan 1 2018\")\n",
    "ax1.set_ylabel(\"$\\\\bar \\zeta$\")\n",
    "ax1.legend(loc=[1.04, 0.36]) # loc=[0, -1])\n",
    "plt.ylim([-2.8, 2.8])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.hist(test_k_averages[test_ts < lt], color=\"m\", alpha=0.4, label=\"pre-lockdown\", density=True)\n",
    "plt.hist(test_k_averages[(lt < test_ts) & (test_ts < lt + 60 * scale_t)], color=\"y\", alpha=0.4, label=\"lockdown\", density=True)\n",
    "plt.hist(train_k_averages, color=\"c\", alpha=0.4, label=\"train\", density=True)\n",
    "plt.axvline(zeta_crit, label=\"$\\\\bar \\zeta^*$\", color=\"gray\", linestyle=\"--\")\n",
    "plt.axvline(-zeta_crit, color=\"gray\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"gray\", linestyle=\"-\")\n",
    "ax2.set_ylabel(\"density\")\n",
    "ax2.set_xlabel(\"$\\\\bar \\zeta$\")\n",
    "plt.ylim([0, 2.7])\n",
    "plt.xlim([-2.8, 2.8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zeta_\" + station + \".pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Map figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "k=168\n",
    "quality_thresh=0.5\n",
    "explained_var_thresh=0.9\n",
    "\n",
    "country1_region = \"madrid-barcelona\"\n",
    "country1_stations = get_stations(country1_region, quality_thresh=quality_thresh)\n",
    "country1_train_ts, country1_test_ts, country1_train_zeta_avg_mat, country1_test_zeta_avg_mat, train_end_t = get_zeta_series(country1_stations, k=k, show=False)\n",
    "train_mahalanobis_dist, test_mahalanobis_dist, num_PCs = get_mahalanobis_dist(country1_train_ts, country1_test_ts,\n",
    "            country1_train_zeta_avg_mat, country1_test_zeta_avg_mat, train_end_t, explained_var_thresh=explained_var_thresh, show=False)\n",
    "cdf = get_nd_cdf(n=num_PCs, radial_density=norm)\n",
    "country1_train_probs, country1_test_probs = (1 - cdf(abs(train_mahalanobis_dist))), (1 - cdf(abs(test_mahalanobis_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# China + Hong Kong\n",
    "alpha=0.001\n",
    "k=168\n",
    "quality_thresh=0.5\n",
    "explained_var_thresh=0.9\n",
    "\n",
    "country2_region = \"beijing-shanghai-wuhan\"\n",
    "country2_stations = get_stations(country2_region, quality_thresh=quality_thresh)\n",
    "country2_train_ts, country2_test_ts, country2_train_zeta_avg_mat, country2_test_zeta_avg_mat, train_end_t = get_zeta_series(country2_stations, k=k, show=False)\n",
    "train_mahalanobis_dist, test_mahalanobis_dist, num_PCs = get_mahalanobis_dist(country2_train_ts, country2_test_ts,\n",
    "            country2_train_zeta_avg_mat, country2_test_zeta_avg_mat, train_end_t, explained_var_thresh=explained_var_thresh, show=False)\n",
    "cdf = get_nd_cdf(n=num_PCs, radial_density=norm)\n",
    "country2_train_probs, country2_test_probs = (1 - cdf(abs(train_mahalanobis_dist))), (1 - cdf(abs(test_mahalanobis_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"SEDAC_POP_2000-01-01_rgb_1440x720.TIFF\")\n",
    "world = np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "import matplotlib \n",
    "\n",
    "pad = 4\n",
    "bworld = np.zeros([world.shape[0] + pad*2, world.shape[1] + pad*2])\n",
    "bworld[pad:-pad, pad:-pad] = world\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "with open(\"data/metadata.json\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "\n",
    "temp = []\n",
    "for fname in metadata:\n",
    "    temp.append(metadata[fname])\n",
    "    temp[-1][\"fname\"] = fname\n",
    "df_meta = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load analysis for major cities\n",
    "\n",
    "# with open(\"all_test_ts.pkl\", \"rb\") as f:\n",
    "#     all_test_ts = pickle.load(f).to_dict()\n",
    "# with open(\"all_test_probs.pkl\", \"rb\") as f:\n",
    "#     all_test_probs = pickle.load(f).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cmap = truncate_colormap(plt.get_cmap('plasma'), 0, 0.95)\n",
    "prob_vmin = -5\n",
    "prob_vmax = 0\n",
    "snap1_date = dt.datetime(2020, 2, 1)\n",
    "snap2_date = dt.datetime(2020, 5, 1)\n",
    "snap1t = time.mktime(snap1_date.timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "snap2t = time.mktime(snap2_date.timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scale = 24* 60 * 60\n",
    "offset = 365 * 2\n",
    "plt.figure(figsize=(20, 3), dpi=200)\n",
    "\n",
    "# storm gloria in spain Jan 20\n",
    "gloria_t = time.mktime(dt.datetime(2020, 1, 19).timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "gloria_end_t = time.mktime(dt.datetime(2020, 1, 26).timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "plt.fill_between([gloria_t / t_scale - offset, gloria_end_t / t_scale - offset], [0, 0], [11, 11], color=\"b\", alpha=0.2, label=\"Storm Gloria\")\n",
    "lt = time.mktime(lockdown_dates[\"barcelona\"].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "plt.fill_between([lt / t_scale - offset, lt / t_scale - offset + 100], [0, 0], [11, 11], color=\"y\", alpha=0.2, label=\"lockdown\")\n",
    "plt.axvline(snap1t / t_scale - offset, linestyle=\"-\", color=\"gray\")\n",
    "plt.axvline(snap2t / t_scale - offset, linestyle=\"-\", color=\"gray\")\n",
    "colors=\"cmyrgb\"\n",
    "i=0\n",
    "plt.plot(country1_test_ts / t_scale - offset, country1_test_probs, linewidth=2, color=\"c\")\n",
    "plt.axhline(alpha, color=\"gray\", linestyle=\"--\", label=\"$\\\\alpha$\")\n",
    "# plt.ylim([10**-6, 1])\n",
    "plt.xlim([-10, 135])\n",
    "plt.semilogy()\n",
    "plt.legend()\n",
    "plt.xlabel(\"days since Jan 1 2020\")\n",
    "plt.ylabel(\"p-value\")\n",
    "\n",
    "# for i, region in enumerate(test_ts):\n",
    "#     # _tr_t = np.log10(all_train_ts[region])\n",
    "#     # _te_t = np.log10(all_test_ts[region])\n",
    "#     # tr_t = 360 * (_tr_t - _tr_t.min()) / (_te_t.max() - _tr_t.min()) - 180\n",
    "#     # te_t = 360 * (_te_t - _tr_t.min()) / (_te_t.max() - _tr_t.min()) - 180\n",
    "#     # _tr_p = np.log10(train_probs[region])\n",
    "#     # _te_p = np.log10(test_probs[region])\n",
    "#     # tr_p = 30 * (_tr_p - _te_p.min()) / (_tr_p.max() - _te_p.min()) + 90\n",
    "#     # te_p = 30 * (_te_p - _te_p.min()) / (_tr_p.max() - _te_p.min()) + 90\n",
    "#     tr_p = train_probs[region]\n",
    "#     te_p = test_probs[region]\n",
    "#     tr_t = all_train_ts[region]\n",
    "#     te_t = all_test_ts[region]\n",
    "#     scale_t = 60 * 60 * 24\n",
    "#     ax1.plot(tr_t / scale_t, tr_p, color=colors[i], label=city_names[region])\n",
    "#     ax1.plot(te_t / scale_t, te_p, color=colors[i])\n",
    "#     lt = time.mktime(lockdown_dates[region].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "#     ax1.axvline(lt / scale_t, color=colors[i], linestyle=\"--\")\n",
    "#     ax1.semilogy()\n",
    "#     ax1.set_ylabel(\"p-value\")\n",
    "#     ax1.set_xlabel(\"days since Jan 1 2018\")\n",
    "# ax1.axvline(365 * 2, color=\"gray\")\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=[0, 0])\n",
    "plt.gca().set_ylim(top=3)\n",
    "plt.title(\"Spain\")\n",
    "plt.savefig(\"spain_p.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scale = 24* 60 * 60\n",
    "offset = 365 * 2\n",
    "plt.figure(figsize=(20, 3), dpi=200)\n",
    "lt = time.mktime(lockdown_dates[\"wuhan\"].timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "lt_end = time.mktime(dt.datetime(2020, 3, 22).timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "plt.fill_between([lt / t_scale - offset, lt_end / t_scale - offset], [0, 0], [11, 11], color=\"y\", alpha=0.2, label=\"Wuhan lockdown\")\n",
    "chinese_ny_start_date = dt.datetime(2020, 1, 25)\n",
    "cnyt = time.mktime(chinese_ny_start_date.timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "plt.fill_between([cnyt / t_scale - offset, cnyt / t_scale - offset + 15], [0, 0], [11, 11], color=\"red\", alpha=0.2, label=\"Chinese New Year\")\n",
    "plt.axvline(snap1t / t_scale - offset, linestyle=\"-\", color=\"gray\")\n",
    "plt.axvline(snap2t / t_scale - offset, linestyle=\"-\", color=\"gray\")\n",
    "\n",
    "plt.plot(country2_test_ts / t_scale - offset, country2_test_probs, linewidth=2, color=\"c\")\n",
    "plt.axhline(alpha, color=\"gray\", linestyle=\"--\", label=\"$\\\\alpha$\")\n",
    "plt.xlim([-10, 135])\n",
    "plt.semilogy()\n",
    "plt.legend()\n",
    "plt.xlabel(\"days since Jan 1 2020\")\n",
    "plt.ylabel(\"p-value\")\n",
    "\n",
    "plt.title(\"China\")\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=[0, 0])\n",
    "plt.gca().set_ylim(top=3)\n",
    "plt.savefig(\"china_p.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 25}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# snapshot_dates = [dt.datetime(2020, 2, 1), dt.datetime(2020, 3, 20), dt.datetime(2020, 5, 1)]\n",
    "snapshot_date = snap1_date\n",
    "snapshot_t = time.mktime(snapshot_date.timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "test_ts = all_test_ts[0.9][168]  # dict of region: test_ts\n",
    "test_probs = all_test_probs[0.9][168]  # dict of region: test_probs\n",
    "city_locs = []\n",
    "city_probs = []\n",
    "for region in test_ts:\n",
    "    idxs = np.nonzero(test_ts[region] >= snapshot_t)[0]\n",
    "    if len(idxs) != 0:\n",
    "        idx = idxs.min()\n",
    "    else:\n",
    "        continue\n",
    "    city_probs.append(test_probs[region][idx])\n",
    "    city_locs.append(region_centers[region])\n",
    "city_locs = np.array(city_locs)\n",
    "\n",
    "plt.figure(dpi=1000, figsize=(20, 15))\n",
    "plt.imshow(bworld, cmap=truncate_colormap(plt.get_cmap('bone'), 0.4, 0.75), vmin=0, vmax=255, extent=(-180, 180, -90, 90), aspect=\"equal\")\n",
    "city_stations = df_meta[(df_meta[\"quality\"] > quality_thresh) & (df_meta[\"mean\"] > obs_thresh) & (df_meta[\"obs\"])]\n",
    "locs = np.array([[df_meta[\"lat\"].values, df_meta[\"lon\"].values]])\n",
    "plt.scatter(locs[:, 1], locs[:, 0], s=0.4, c=\"white\", alpha=0.4)\n",
    "sc = plt.scatter(city_locs[:, 1], city_locs[:, 0], s=120, c=np.log10(city_probs), alpha=1, cmap=prob_cmap, vmin=prob_vmin, vmax=prob_vmax)\n",
    "plt.axis('off')\n",
    "# plt.subplots_adjust(hspace=-0.5)\n",
    "plt.tight_layout()\n",
    "plt.colorbar(sc, label=\"$\\log_{10}(p)$\", shrink=0.5, location=\"right\")\n",
    "plt.savefig(\"world_map\" + str(snapshot_date)[:10] + \".svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white dot for each station, colored dots for a few cities in which we've done the analysis, plot time-series for hierarchy\n",
    "\n",
    "\n",
    "layout = \\\n",
    "\"\"\"\n",
    "AAAA\n",
    "BBBB\n",
    "CCCC\n",
    "CCCC\n",
    "\"\"\"\n",
    "fig, axs = plt.subplot_mosaic(layout, dpi=1500, figsize=(15, 10))\n",
    "axnames = \"ABC\"\n",
    "snapshot_dates = [dt.datetime(2020, 2, 1), dt.datetime(2020, 3, 20), dt.datetime(2020, 5, 1)]\n",
    "for i, snapshot_date in enumerate(snapshot_dates):\n",
    "    snapshot_t = time.mktime(snapshot_date.timetuple()) - time.mktime(dt.datetime(2018, 1, 1).timetuple())\n",
    "    test_ts = all_test_ts[0.9][168]  # dict of region: test_ts\n",
    "    test_probs = all_test_probs[0.9][168]  # dict of region: test_probs\n",
    "    city_locs = []\n",
    "    city_probs = []\n",
    "    for region in test_ts:\n",
    "        idxs = np.nonzero(test_ts[region] >= snapshot_t)[0]\n",
    "        if len(idxs) != 0:\n",
    "            idx = idxs.min()\n",
    "        else:\n",
    "            continue\n",
    "        city_probs.append(test_probs[region][idx])\n",
    "        city_locs.append(region_centers[region])\n",
    "    city_locs = np.array(city_locs)\n",
    "    \n",
    "    plt.sca(axs[axnames[i]])\n",
    "    if i < len(snapshot_dates) - 1:\n",
    "        plt.imshow(world[:world.shape[0] // 2, :], cmap=truncate_colormap(plt.get_cmap('bone'), 0.4, 0.75), vmin=0, vmax=255, extent=(-180, 180, 0, 90), aspect=\"equal\")\n",
    "    else:\n",
    "        plt.imshow(world, cmap=truncate_colormap(plt.get_cmap('bone'), 0.4, 0.75), vmin=0, vmax=255, extent=(-180, 180, -90, 90), aspect=\"equal\")\n",
    "    city_stations = df_meta[(df_meta[\"quality\"] > quality_thresh) & (df_meta[\"mean\"] > obs_thresh) & (df_meta[\"obs\"])]\n",
    "    locs = np.array([[df_meta[\"lat\"].values, df_meta[\"lon\"].values]])\n",
    "    plt.scatter(locs[:, 1], locs[:, 0], s=0.2, c=\"white\", alpha=0.4)\n",
    "    sc = plt.scatter(city_locs[:, 1], city_locs[:, 0], s=50, c=np.log10(city_probs), alpha=1, cmap=truncate_colormap(plt.get_cmap('plasma'), 0, 0.9), vmin=-5, vmax=0)\n",
    "    plt.axis('off')\n",
    "# plt.subplots_adjust(hspace=-0.5)\n",
    "plt.tight_layout()\n",
    "plt.colorbar(sc, ax=[ax for ax in axs.values()], label=\"$\\log_{10}(p)$\", shrink=0.6, location=\"right\")\n",
    "plt.savefig(\"world_map\" + \",\".join([str(s)[:10] for s in snapshot_dates]) + \".svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "268886bf53b46c7c8da095f9817150a082264fd7a3d3273094d11ff2d3fbd287"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
